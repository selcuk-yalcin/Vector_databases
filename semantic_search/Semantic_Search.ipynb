{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77448887-ed71-48e1-bf0d-2ff499d0c7ca",
   "metadata": {},
   "source": [
    "# Semantic Search - OHS Legislation (Turkish)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750cd7dd",
   "metadata": {},
   "source": [
    "### Import the Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809aa032-d737-450d-aafa-e32bfba9d8f8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965e4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9e622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../../DLAIUtils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /Users/selcuk/miniconda3/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: google-api-python-client in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.188.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.47.0)\n",
      "Requirement already satisfied: protobuf in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.12.5)\n",
      "Requirement already satisfied: tqdm in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: google-api-python-client in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.188.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.47.0)\n",
      "Requirement already satisfied: protobuf in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (2.12.5)\n",
      "Requirement already satisfied: tqdm in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.31.1)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.31.1)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2023.7.22)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/selcuk/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-dotenv\n",
    "#!pip install datasets==2.15.0\n",
    "#!pip install PyPDF2\n",
    "#!pip install openai\n",
    "#!pip install google-generativeai\n",
    "!pip install -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30698fb9-4709-4088-9905-9ccb4efd5e09",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from DLAIUtils import Utils\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab484bb-3bfb-4c52-a5bd-bcbe4a7a63d2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92fc2d",
   "metadata": {},
   "source": [
    "### Load the OHS Legislation PDF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce859e4b-9b50-4f53-b357-28d3e3872c87",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters extracted: 234007\n",
      "First 500 characters:\n",
      "Bu dokÃ¼man Global Belgelendirme Hizmetleri A.Åž. tarafÄ±ndan 01.06.2021 tarihinde oluÅŸturulmuÅŸtur.  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "4857 SAYILI Ä°Åž KANUNU  \n",
      "6331 SAYILI Ä°Åž SAÄžLIÄžI VE GÃœVENLÄ°ÄžÄ° \n",
      "KANUNU  \n",
      "BAÄžLI DÄ°ÄžER MEVZUAT  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1 \n",
      " Ä°Ã‡Ä°NDEKÄ°LER  \n",
      "4857 SAYILI Ä°Åž KANUNU  ...........................................................................................................  4 \n",
      "Alt Ä°ÅŸverenlik YÃ¶netmeliÄŸi  .......................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Load ISG Mevzuat PDF\n",
    "pdf_path = 'data/isg_mevzuat.pdf'\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Extract text from PDF\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "print(f\"Total characters extracted: {len(pdf_text)}\")\n",
    "print(f\"First 500 characters:\\n{pdf_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356d4112-fa51-4092-9841-8b266e3b6a2c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 405\n",
      "\n",
      "First 3 chunks:\n",
      "\n",
      "--- Chunk 1 (length: 699) ---\n",
      "Bu dokÃ¼man Global Belgelendirme Hizmetleri A.Åž tarafÄ±ndan 01.06.2021 tarihinde oluÅŸturulmuÅŸtur 4857 SAYILI Ä°Åž KANUNU  \n",
      "6331 SAYILI Ä°Åž SAÄžLIÄžI VE GÃœVENLÄ°ÄžÄ° \n",
      "KANUNU  \n",
      "BAÄžLI DÄ°ÄžER MEVZUAT  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1 \n",
      " Ä°Ã‡Ä°NDEKÄ°LER  \n",
      "4857 SAYILI Ä°Åž KANUNU  .....................................................\n",
      "\n",
      "--- Chunk 2 (length: 779) ---\n",
      "11 Ã‡ocuk ve GenÃ§ Ä°ÅŸÃ§ilerin Ã‡alÄ±ÅŸtÄ±rÄ±lma Usul ve EsaslarÄ± HakkÄ±nda YÃ¶netmelik ............................ 12 \n",
      "Gebe veya Emziren KadÄ±nlarÄ±n Ã‡alÄ±ÅŸtÄ±rÄ±lma ÅžartlarÄ±yla Ä°lgili YÃ¶netmelik  .................................. 13 \n",
      "HaftalÄ±k Ä°ÅŸ GÃ¼nlerine BÃ¶lÃ¼nemeyen Ã‡alÄ±ÅŸma SÃ¼releri YÃ¶netmeliÄŸi  .................\n",
      "\n",
      "--- Chunk 3 (length: 751) ---\n",
      "Ã‡alÄ±ÅŸma YÃ¶netmeliÄŸi .............................. 17 KadÄ±n Ã‡a lÄ±ÅŸanlarÄ±n Gece PostalarÄ±nda Ã‡alÄ±ÅŸtÄ±rÄ±lma KoÅŸullarÄ± HakkÄ±nda YÃ¶netmelik .................. 18 \n",
      "Postalar Halinde Ä°ÅŸÃ§i Ã‡alÄ±ÅŸtÄ±rÄ±larak YÃ¼rÃ¼tÃ¼len Ä°ÅŸlerde Ã‡alÄ±ÅŸmalara Ä°liÅŸkin YÃ¶netmelik  ............... 19 \n",
      "Uzaktan Ã‡alÄ±ÅŸma YÃ¶netmeliÄŸi  .........\n"
     ]
    }
   ],
   "source": [
    "# Improved text chunking to preserve complete sentences\n",
    "def split_text_into_chunks(text, chunk_size=800, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks at sentence boundaries\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Split into sentences (Turkish-aware)\n",
    "    sentence_endings = r'[.!?]\\s+'\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # If adding this sentence exceeds chunk_size and we have content, save chunk\n",
    "        if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Keep overlap: start new chunk with last part of previous chunk\n",
    "            words = current_chunk.split()\n",
    "            overlap_words = words[-int(len(words) * 0.2):] if len(words) > 5 else []\n",
    "            current_chunk = ' '.join(overlap_words) + ' ' + sentence\n",
    "        else:\n",
    "            current_chunk += ' ' + sentence\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks from PDF text with better sentence preservation\n",
    "text_chunks = split_text_into_chunks(pdf_text, chunk_size=800, overlap=200)\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")\n",
    "print(f\"\\nFirst 3 chunks:\")\n",
    "for i, chunk in enumerate(text_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} (length: {len(chunk)}) ---\")\n",
    "    print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332d1241-61ae-4d09-bf46-52081c133c0c",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cleaned chunks: 405\n",
      "\n",
      "First 2 chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Bu dokÃ¼man Global Belgelendirme Hizmetleri A.Åž tarafÄ±ndan 01.06.2021 tarihinde oluÅŸturulmuÅŸtur 4857 SAYILI Ä°Åž KANUNU  \n",
      "6331 SAYILI Ä°Åž SAÄžLIÄžI VE GÃœVENLÄ°ÄžÄ° \n",
      "KANUNU  \n",
      "BAÄžLI DÄ°ÄžER MEVZUAT  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1 \n",
      " Ä°Ã‡Ä°NDEKÄ°LER  \n",
      "4857 SAYILI Ä°Åž KANUNU  .....................................................\n",
      "\n",
      "--- Chunk 2 ---\n",
      "11 Ã‡ocuk ve GenÃ§ Ä°ÅŸÃ§ilerin Ã‡alÄ±ÅŸtÄ±rÄ±lma Usul ve EsaslarÄ± HakkÄ±nda YÃ¶netmelik ............................ 12 \n",
      "Gebe veya Emziren KadÄ±nlarÄ±n Ã‡alÄ±ÅŸtÄ±rÄ±lma ÅžartlarÄ±yla Ä°lgili YÃ¶netmelik  .................................. 13 \n",
      "HaftalÄ±k Ä°ÅŸ GÃ¼nlerine BÃ¶lÃ¼nemeyen Ã‡alÄ±ÅŸma SÃ¼releri YÃ¶netmeliÄŸi  .................\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare ISG mevzuat chunks\n",
    "def clean_text(text):\n",
    "    # Replace problematic characters with ASCII equivalents\n",
    "    text = text.replace('â€“', '-')  # en dash\n",
    "    text = text.replace('â€”', '-')  # em dash\n",
    "    text = text.replace(''', \"'\")  # left single quotation mark\n",
    "    text = text.replace(''', \"'\")  # right single quotation mark\n",
    "    text = text.replace('\"', '\"')  # left double quotation mark\n",
    "    text = text.replace('\"', '\"')  # right double quotation mark\n",
    "    # Keep Turkish characters but remove other problematic Unicode\n",
    "    # Don't encode to ASCII for Turkish text\n",
    "    return text\n",
    "\n",
    "# Clean the chunks\n",
    "cleaned_chunks = [clean_text(chunk) for chunk in text_chunks if len(chunk.strip()) > 50]\n",
    "\n",
    "print(f'Number of cleaned chunks: {len(cleaned_chunks)}')\n",
    "print('\\nFirst 2 chunks:')\n",
    "for i, chunk in enumerate(cleaned_chunks[:2]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c0402",
   "metadata": {},
   "source": [
    "### Check cuda and Setup the model\n",
    "\n",
    "**Note**: \"Checking cuda\" refers to checking if you have access to GPUs (faster compute). In this course, we are using CPUs. So, you might notice some code cells taking a little longer to run.\n",
    "\n",
    "We are using *all-MiniLM-L6-v2* sentence-transformers model that maps sentences to a 384 dimensional dense vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fb67759-ab38-4472-bfb0-4a56d1c05955",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry no cuda.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device != 'cuda':\n",
    "    print('Sorry no cuda.')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90ec5ec-5397-4ed5-8163-7a901b6ecb0c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'iÅŸÃ§i saÄŸlÄ±ÄŸÄ± ve gÃ¼venliÄŸi nedir?'\n",
    "xq = model.encode(query)\n",
    "xq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b5fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780a189",
   "metadata": {},
   "source": [
    "### Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e3a94e-127f-4667-a9ae-7a17d7304ee6",
   "metadata": {
    "height": 43
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API keys loaded from .env file (OpenAI + Google Gemini)\n"
     ]
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Verify Pinecone API key is loaded\n",
    "if not PINECONE_API_KEY or PINECONE_API_KEY == 'your_pinecone_api_key_here':\n",
    "    raise ValueError(\"Please set PINECONE_API_KEY in .env file\")\n",
    "\n",
    "# Configure Google Gemini\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    print(\"âœ“ API keys loaded from .env file (OpenAI + Google Gemini)\")\n",
    "else:\n",
    "    print(\"âœ“ API keys loaded from .env file (OpenAI only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a75eca-60f0-478b-bdcf-b68732c1545d",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing index: isg-86hbriq-qif5lhm6p-ygourxdueztik7oaqa\n",
      "Creating new index: isg-86hbriq-qif5lhm6p-ygourxdueztik7oaqa\n",
      "Creating new index: isg-86hbriq-qif5lhm6p-ygourxdueztik7oaqa\n",
      "Waiting for index to be ready...\n",
      "Waiting for index to be ready...\n",
      "âœ“ Index ready: <pinecone.db_data.index.Index object at 0x3070be350>\n",
      "âœ“ Index ready: <pinecone.db_data.index.Index object at 0x3070be350>\n",
      "âœ“ Index stats: {'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '150',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Wed, 14 Jan 2026 18:47:39 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '43',\n",
      "                                    'x-pinecone-request-id': '5770634670681135803',\n",
      "                                    'x-pinecone-request-latency-ms': '43',\n",
      "                                    'x-pinecone-response-duration-ms': '44'}},\n",
      " 'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n",
      "âœ“ Index stats: {'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '150',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Wed, 14 Jan 2026 18:47:39 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '43',\n",
      "                                    'x-pinecone-request-id': '5770634670681135803',\n",
      "                                    'x-pinecone-request-latency-ms': '43',\n",
      "                                    'x-pinecone-response-duration-ms': '44'}},\n",
      " 'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "INDEX_NAME = utils.create_dlai_index_name('isg')\n",
    "\n",
    "# Check and delete existing index if present\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    print(f\"Deleting existing index: {INDEX_NAME}\")\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "    time.sleep(5)  # Wait for deletion to complete\n",
    "\n",
    "print(f\"Creating new index: {INDEX_NAME}\")\n",
    "pinecone.create_index(\n",
    "    name=INDEX_NAME, \n",
    "    dimension=model.get_sentence_embedding_dimension(), \n",
    "    metric='cosine',\n",
    "    spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    ")\n",
    "\n",
    "# Wait for index to be ready\n",
    "print(\"Waiting for index to be ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Connect to index\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "print(f\"âœ“ Index ready: {index}\")\n",
    "print(f\"âœ“ Index stats: {index.describe_index_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d9424",
   "metadata": {},
   "source": [
    "### Create Embeddings and Upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea551303-aa81-41cd-adc5-dc9ea8072397",
   "metadata": {
    "height": 352
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to process: 405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61b62e0eba0490fb35ed66c836844f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Successfully uploaded 405 ISG mevzuat chunks to Pinecone\n",
      "Text mapping created with 405 entries\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "# Use all chunks from ISG mevzuat\n",
    "isg_chunks = cleaned_chunks\n",
    "\n",
    "# Create a mapping to store the actual text (for later retrieval)\n",
    "text_mapping = {i: text for i, text in enumerate(isg_chunks)}\n",
    "\n",
    "print(f\"Total chunks to process: {len(isg_chunks)}\")\n",
    "\n",
    "for i in tqdm(range(0, len(isg_chunks), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i + batch_size, len(isg_chunks))\n",
    "    # create IDs batch\n",
    "    ids = [f\"isg_{x}\" for x in range(i, i_end)]\n",
    "    # create embeddings\n",
    "    xc = model.encode(isg_chunks[i:i_end])\n",
    "    # create metadata batch - store only chunk_id to avoid encoding issues\n",
    "    metadatas = [{'chunk_id': idx} for idx in range(i, i_end)]\n",
    "    # create records list for upsert with proper format\n",
    "    records = [\n",
    "        {\n",
    "            'id': id_,\n",
    "            'values': values.tolist(),\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        for id_, values, metadata in zip(ids, xc, metadatas)\n",
    "    ]\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=records)\n",
    "\n",
    "print(f\"\\nâœ“ Successfully uploaded {len(isg_chunks)} ISG mevzuat chunks to Pinecone\")\n",
    "print(f\"Text mapping created with {len(text_mapping)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6153920a-f4c4-420e-9790-262dd2299fc6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
       "                                    'content-length': '185',\n",
       "                                    'content-type': 'application/json',\n",
       "                                    'date': 'Wed, 14 Jan 2026 18:47:49 GMT',\n",
       "                                    'grpc-status': '0',\n",
       "                                    'server': 'envoy',\n",
       "                                    'x-envoy-upstream-service-time': '4',\n",
       "                                    'x-pinecone-request-id': '5054425798342982642',\n",
       "                                    'x-pinecone-request-latency-ms': '3',\n",
       "                                    'x-pinecone-response-duration-ms': '5'}},\n",
       " 'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'memoryFullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'__default__': {'vector_count': 405}},\n",
       " 'storageFullness': 0.0,\n",
       " 'total_vector_count': 405,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71135418",
   "metadata": {},
   "source": [
    "### Run Your Queries - Multi-AI Provider Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28b20b81-4782-4ce2-aec3-9576c7779f2e",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Enhanced query function with multiple AI providers and response length control\n",
    "def run_query(query, use_ai=True, ai_provider=\"openai\", use_reasoning=False, max_length=\"normal\", top_k=5):\n",
    "    \"\"\"\n",
    "    Query ISG legislation with semantic search and optional AI enhancement\n",
    "    \n",
    "    Args:\n",
    "        query: The question to ask (in Turkish)\n",
    "        use_ai: Whether to use AI for response synthesis (default: True)\n",
    "        ai_provider: \"openai\", \"gemini\", or \"none\" (default: \"openai\")\n",
    "        use_reasoning: Use advanced reasoning models (default: False)\n",
    "        max_length: \"short\" (3-4 sentences), \"medium\" (5-7 sentences), or \"normal\" (detailed)\n",
    "        top_k: Number of relevant chunks to retrieve (default: 5)\n",
    "    \"\"\"\n",
    "    embedding = model.encode(query).tolist()\n",
    "    results = index.query(top_k=top_k, vector=embedding, include_metadata=True, include_values=False)\n",
    "    \n",
    "    if not use_ai or ai_provider == \"none\":\n",
    "        # Simple mode: just print the chunks\n",
    "        print(\"ðŸ” Most Relevant Results:\\n\")\n",
    "        for i, result in enumerate(results['matches'], 1):\n",
    "            chunk_id = result['metadata']['chunk_id']\n",
    "            text = text_mapping.get(chunk_id, \"Text not found\")\n",
    "            print(f\"{i}. [Score: {round(result['score'], 2)}]\\n{text}\\n\")\n",
    "    else:\n",
    "        # AI mode: use AI to synthesize a better answer\n",
    "        if ai_provider == \"gemini\":\n",
    "            if use_reasoning:\n",
    "                print(\"ðŸ§  Deep Analysis with Gemini 2.5 Pro:\\n\")\n",
    "            else:\n",
    "                print(\"ðŸ¤– Enhanced Response with Gemini 2.5 Flash:\\n\")\n",
    "        else:  # openai\n",
    "            if use_reasoning:\n",
    "                print(\"ðŸ§  Deep Analysis with GPT-4 (o1 Reasoning):\\n\")\n",
    "            else:\n",
    "                print(\"ðŸ¤– Enhanced Response with GPT-4 mini:\\n\")\n",
    "        \n",
    "        # Gather relevant chunks\n",
    "        relevant_texts = []\n",
    "        for result in results['matches']:\n",
    "            chunk_id = result['metadata']['chunk_id']\n",
    "            text = text_mapping.get(chunk_id, \"\")\n",
    "            if text:\n",
    "                relevant_texts.append(text)\n",
    "        \n",
    "        context = \"\\n\\n\".join(relevant_texts)\n",
    "        \n",
    "        # Length-aware instructions\n",
    "        if max_length == \"short\":\n",
    "            length_instruction = \"VERY IMPORTANT: Limit your response to maximum 3-4 sentences. Only mention the most important points.\"\n",
    "            max_tokens = 2048\n",
    "        elif max_length == \"medium\":\n",
    "            length_instruction = \"Keep your response concise (5-7 sentences).\"\n",
    "            max_tokens = 3072\n",
    "        else:  # normal or long\n",
    "            length_instruction = \"Provide a detailed and complete response.\"\n",
    "            max_tokens = 4096\n",
    "        \n",
    "        system_prompt = f\"\"\"You are an expert on OHS (Occupational Health and Safety) legislation. \n",
    "Answer questions based ONLY on the provided legislation texts. \n",
    "Provide answers in proper Turkish, in a bullet-point format, and reference relevant articles.\n",
    "{length_instruction}\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Answer the question based on the following OHS legislation texts.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Legislation Texts:\n",
    "{context}\n",
    "\n",
    "Please:\n",
    "1. Answer the question fully and clearly\n",
    "2. Use ONLY the information from the provided legislation texts\n",
    "3. Provide the answer in proper Turkish, in bullet-point format\n",
    "4. Reference relevant articles\n",
    "5. Give practical examples\n",
    "6. Highlight important points\n",
    "{length_instruction}\"\"\"\n",
    "\n",
    "        # Call AI provider\n",
    "        if ai_provider == \"gemini\":\n",
    "            # Use Gemini 1.5 Flash for both normal and reasoning (FREE!)\n",
    "            # Note: Using same model for both to avoid quota limits\n",
    "            gemini_model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
    "            response = gemini_model.generate_content(\n",
    "                f\"{system_prompt}\\n\\n{user_prompt}\",\n",
    "                generation_config={\n",
    "                    'temperature': 0.3,\n",
    "                    'max_output_tokens': max_tokens,\n",
    "                    'top_p': 0.95,\n",
    "                    'top_k': 40\n",
    "                }\n",
    "            )\n",
    "            if use_reasoning:\n",
    "                print(\"ðŸ’­ Used Gemini 2.5 Flash (with reasoning mode)\\n\")\n",
    "            else:\n",
    "                print(\"ðŸ¤– Enhanced Response with Gemini 2.5 Flash:\\n\")\n",
    "            print(response.text)\n",
    "        else:  # openai\n",
    "            if use_reasoning:\n",
    "                # Use o1 model for deep reasoning\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=\"o1-mini\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": f\"{system_prompt}\\n\\n{user_prompt}\"}\n",
    "                    ]\n",
    "                )\n",
    "                print(\"ðŸ’­ Used O1 Reasoning Model (with internal reasoning)\\n\")\n",
    "                print(response.choices[0].message.content)\n",
    "            else:\n",
    "                # Use GPT-4 for standard response\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "                print(response.choices[0].message.content)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ðŸ“š Source Texts (showing top {min(3, len(relevant_texts))}):\")\n",
    "        for i, text in enumerate(relevant_texts[:3], 1):\n",
    "            print(f\"\\n{i}. {text[:200]}...\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03372d2d",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test SorularÄ±\n",
    "\n",
    "AÅŸaÄŸÄ±da Ä°SG mevzuatÄ± ile ilgili Ã¶rnek sorular bulunmaktadÄ±r. Her hÃ¼cre baÄŸÄ±msÄ±z olarak Ã§alÄ±ÅŸtÄ±rÄ±labilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50724f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soru 1: Kimyasal maddelerle Ã§alÄ±ÅŸma\n",
    "query_1 = \"Kimyasal maddelerle Ã§alÄ±ÅŸÄ±lan bir iÅŸyerinde, Ã§alÄ±ÅŸanlarÄ±n maruziyetini Ã¶nlemek iÃ§in alÄ±nmasÄ± gereken Ã¶ncelikli tedbirler (ikame, teknik Ã¶nlemler vb.) sÄ±rasÄ±yla nelerdir?\"\n",
    "\n",
    "run_query(query_1, use_ai=True, ai_provider=\"openai\", max_length=\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soru 2: Risk deÄŸerlendirmesi sÃ¼reci\n",
    "query_2 = \"Bir iÅŸ yerinde risk deÄŸerlendirmesi yapÄ±lÄ±rken hangi adÄ±mlar izlenmeli ve kimler bu ekibe dahil edilmelidir?\"\n",
    "\n",
    "run_query(query_2, use_ai=True, ai_provider=\"openai\", max_length=\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soru 3: Ä°ÅŸveren yÃ¼kÃ¼mlÃ¼lÃ¼kleri\n",
    "query_3 = \"Ä°ÅŸverenin iÅŸ saÄŸlÄ±ÄŸÄ± ve gÃ¼venliÄŸi konusundaki genel yÃ¼kÃ¼mlÃ¼lÃ¼kleri maddeler halinde nelerdir?\"\n",
    "\n",
    "run_query(query_3, use_ai=True, ai_provider=\"openai\", max_length=\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soru 4: Ä°ÅŸ kazasÄ± ve acil durum planÄ±\n",
    "query_4 = \"Ä°ÅŸ yerlerinde acil durum planÄ± hazÄ±rlanÄ±rken hangi hususlar dikkate alÄ±nmalÄ± ve tatbikatlar ne sÄ±klÄ±kla yapÄ±lmalÄ±dÄ±r?\"\n",
    "\n",
    "run_query(query_4, use_ai=True, ai_provider=\"openai\", max_length=\"long\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
